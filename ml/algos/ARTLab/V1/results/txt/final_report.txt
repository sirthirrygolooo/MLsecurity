=== Adversarial Robustness Experiment Report ===

Key Findings:
- The model's accuracy drops from 77.91% to 41.73% under FGSM attack (46.44% reduction)
- Under more sophisticated PGD attack, accuracy drops further to 26.48%
- Adversarial training improves robustness, reducing FGSM effectiveness by 87.46%
- The trade-off is a 58.76% increase in training time

